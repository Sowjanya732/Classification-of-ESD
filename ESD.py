# -*- coding: utf-8 -*-
"""ESD

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVTW0NrMuDlp3bWNe3qnZGr8v_W9nqw4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_selection import RFECV
from imblearn.combine import SMOTEENN
from collections import Counter

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/dermatology_database_new (2).csv')

# Display first few rows
print(df.head())

# Show dataset dimensions (rows, columns)
print("Dataset shape:", df.shape)

# Show column names and data types
print(df.info())

# Show summary statistics (mean, min, max, etc.)
print(df.describe())

# Check for missing values in each column
print(df.isnull().sum())

# Show number of unique values per column
print(df.nunique())

# Check class distribution (if classification task)
if 'class' in df.columns:
    print(df['class'].value_counts())

# Convert 'age' to numeric and handle missing values
df['age'] = pd.to_numeric(df['age'], errors='coerce')
df['age'] = df['age'].fillna(df['age'].median())

# Separate features and target
X = df.drop('class', axis=1)
y = df['class']

# Fix the class labels to start from 0
y = y - 1

# Feature Selection using RFECV
rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=50, random_state=42), cv=3, n_jobs=-1)
X_selected = rfecv.fit_transform(X, y)

sns.countplot(x=y, palette='viridis')
plt.title("Class Distribution Before Resampling")
plt.show()

# Handle class imbalance
smote_enn = SMOTEENN(random_state=42)
X_resampled, y_resampled = smote_enn.fit_resample(X_selected, y)

print("Class distribution after resampling:", Counter(y_resampled))

sns.countplot(x=y_resampled, palette='viridis')
plt.title("Class Distribution After SMOTEENN")
plt.show()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'KNN': KNeighborsClassifier(),
    'Na√Øve Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(eval_metric='mlogloss', random_state=42),
    'MLP': MLPClassifier(max_iter=1000, random_state=42)
}

# Hyperparameter tuning for selected models
param_distributions = {
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]},
    'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]},
    'XGBoost': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}
}

best_models = {}
for model_name, model in models.items():
    if model_name in param_distributions:
        random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=5, cv=3, scoring='accuracy', n_jobs=-1)
        random_search.fit(X_train_scaled, y_train)
        best_models[model_name] = random_search.best_estimator_
    else:
        model.fit(X_train_scaled, y_train)
        best_models[model_name] = model

# Stacking Classifier
stacking_model = StackingClassifier(
    estimators=[('rf', best_models['Random Forest']), ('gb', best_models['Gradient Boosting'])],
    final_estimator=LogisticRegression(max_iter=1000, random_state=42)
)
stacking_model.fit(X_train_scaled, y_train)
best_models['Stacking Classifier'] = stacking_model

# Evaluate models
metrics_summary = []
for model_name, model in best_models.items():
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)

    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    metrics_summary.append({
        'Model': model_name,
        'Train Accuracy': round(train_accuracy, 4),
        'Test Accuracy': round(test_accuracy, 4),
        'Precision': round(precision, 4),
        'Recall': round(recall, 4),
        'F1-Score': round(f1, 4)
    })

# Visualize Model Performance
metrics_df = pd.DataFrame(metrics_summary)
metrics_df = metrics_df.sort_values(by='Test Accuracy', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Test Accuracy', hue='Model', data=metrics_df, palette='viridis', legend=False)
plt.xticks(rotation=45)
plt.title('Test Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.show()

# Display Model Performance
print("üîç **Model Performance Summary**")
print(metrics_df)

import shap

explainer = shap.Explainer(best_models['Random Forest'])
shap_values = explainer.shap_values(X_test_scaled)
shap.summary_plot(shap_values, X_test_scaled)

